{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aikl0IDtRBOS"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from constants import TEST_DIR, MODEL_PATH, EMOTION_MAP, IMG_DIM\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image as kimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb2QVlMLQ1Ve"
      },
      "source": [
        "def predict_emotion(img, m):\n",
        "    images = []\n",
        "    img = kimg.img_to_array(img)\n",
        "\n",
        "    # Since our model takes batches, we make a batch containing copies of img\n",
        "    img = np.resize(img, (1, 48, 48, 1))\n",
        "    images.append(img)\n",
        "\n",
        "    prediction = m.predict(images)\n",
        "    prediction = np.argmax(prediction, axis=1)\n",
        "\n",
        "    return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr0ahtexROl8"
      },
      "source": [
        "def display_expression(full_img, model, mode=0):\n",
        "    full_img_g = cv2.cvtColor(full_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Use haarcascade to find face in img\n",
        "    face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
        "    faces = face_cascade.detectMultiScale(full_img_g, 1.3, 5)\n",
        "\n",
        "    # Go over all faces found in image and detect emotion\n",
        "    for (x, y, w, h) in faces:\n",
        "\n",
        "        # Draw bounding box on img\n",
        "        full_img = cv2.rectangle(full_img, (x, y), (x + w, y + h), (0, 0, 0), 2)\n",
        "\n",
        "        # Crop out bounding box\n",
        "        full_img_bb = full_img[y:y + h, x:x + w]\n",
        "\n",
        "        # Resize image to 48x48 for our model\n",
        "        resized_image = cv2.resize(full_img_bb, (IMG_DIM, IMG_DIM))\n",
        "        resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        if mode == 0:\n",
        "            cv2.imwrite(\"./Test pics/\" + str(w) + str(h) + '_faces.jpg', resized_image)\n",
        "\n",
        "        # Pass image to model\n",
        "        expression = predict_emotion(resized_image, model)\n",
        "\n",
        "        # Label and display image\n",
        "        cv2.putText(full_img, EMOTION_MAP[expression[0]], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7,\n",
        "                    (0, 255, 0), 2)\n",
        "        cv2.imshow(\"Facial Expression Analysis\", full_img)\n",
        "        cv2.waitKey(mode)\n",
        "\n",
        "        return\n",
        "\n",
        "    print(\"Couldn't find face\")\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgLLlCIyRS3J"
      },
      "source": [
        "\n",
        "def detect_emotions_webcam(model):\n",
        "    webcam = cv2.VideoCapture(0)\n",
        "\n",
        "    while True:\n",
        "        capture, frame = webcam.read()\n",
        "\n",
        "        if capture == False:\n",
        "            continue\n",
        "\n",
        "        # in video mode\n",
        "        display_expression(frame, model, mode=1)\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXMYLj8iRVLP"
      },
      "source": [
        "# Find facial expression in an individual img\n",
        "model = load_model(MODEL_PATH)\n",
        "path = \"./Test pics/got2.jpg\"\n",
        "full_img = cv2.imread(path)\n",
        "display_expression(full_img, model)\n",
        "\n",
        "# detect_emotions_webcam(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2xvBoB9RXPX"
      },
      "source": [
        "# Test our models accuracy across our testing set\n",
        "test = False\n",
        "\n",
        "if test == True:\n",
        "    result=[]\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for emotion in os.listdir(TEST_DIR):\n",
        "\n",
        "        for filename in os.listdir(os.path.join(TEST_DIR, emotion)):\n",
        "            path = os.path.join(os.path.join(TEST_DIR,emotion), filename)\n",
        "            img = kimg.load_img(path, target_size=(48, 48), color_mode=\"grayscale\")\n",
        "\n",
        "            # models prediction\n",
        "            prediction = predict_emotion(img, model)\n",
        "            emotion_str = EMOTION_MAP[prediction[0]]\n",
        "\n",
        "            #if it matches true label\n",
        "            if emotion == emotion_str:\n",
        "                correct+=1\n",
        "\n",
        "            total += 1\n",
        "\n",
        "    print(\"test accuracy: {}\".format(correct/total))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}