{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SHAHMIRALI/Facial-Expression-Analyzer/blob/colab/predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wlw3VdLShY9",
        "outputId": "10c9e70f-fb8c-4727-cf94-be2a41950c7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !git clone https://k-rtik:a433eb39befb48f6853103224266f0af95eba23b@github.com/SHAHMIRALI/Facial-Expression-Analyzer.git\n",
        "!cd Facial-Expression-Analyzer && mv * ../\n",
        "!ls\n",
        "# !cd sample_data && ls"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat '*': No such file or directory\n",
            " constants.py\t Facial-Expression-Analyzer   __pycache__   script.py\n",
            " Datasets\t model.py\t\t      README.md    'Test pics'\n",
            " DLTutorial.py\t predict.py\t\t      sample_data   train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aikl0IDtRBOS"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from constants import TEST_DIR, MODEL_PATH, EMOTION_MAP, IMG_DIM\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image as kimg"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb2QVlMLQ1Ve"
      },
      "source": [
        "def predict_emotion(img, m):\n",
        "    images = []\n",
        "    img = kimg.img_to_array(img)\n",
        "\n",
        "    # Since our model takes batches, we make a batch containing copies of img\n",
        "    img = np.resize(img, (1, 48, 48, 1))\n",
        "    images.append(img)\n",
        "\n",
        "    prediction = m.predict(images)\n",
        "    prediction = np.argmax(prediction, axis=1)\n",
        "\n",
        "    return prediction"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr0ahtexROl8"
      },
      "source": [
        "def display_expression(full_img, model, mode=0):\n",
        "    full_img_g = cv2.cvtColor(full_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Use haarcascade to find face in img\n",
        "    face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
        "    faces = face_cascade.detectMultiScale(full_img_g, 1.3, 5)\n",
        "\n",
        "    # Go over all faces found in image and detect emotion\n",
        "    for (x, y, w, h) in faces:\n",
        "\n",
        "        # Draw bounding box on img\n",
        "        full_img = cv2.rectangle(full_img, (x, y), (x + w, y + h), (0, 0, 0), 2)\n",
        "\n",
        "        # Crop out bounding box\n",
        "        full_img_bb = full_img[y:y + h, x:x + w]\n",
        "\n",
        "        # Resize image to 48x48 for our model\n",
        "        resized_image = cv2.resize(full_img_bb, (IMG_DIM, IMG_DIM))\n",
        "        resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        if mode == 0:\n",
        "            cv2.imwrite(\"./Test pics/\" + str(w) + str(h) + '_faces.jpg', resized_image)\n",
        "\n",
        "        # Pass image to model\n",
        "        expression = predict_emotion(resized_image, model)\n",
        "\n",
        "        # Label and display image\n",
        "        cv2.putText(full_img, EMOTION_MAP[expression[0]], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7,\n",
        "                    (0, 255, 0), 2)\n",
        "        cv2.imshow(\"Facial Expression Analysis\", full_img)\n",
        "        cv2.waitKey(mode)\n",
        "\n",
        "        return\n",
        "\n",
        "    print(\"Couldn't find face\")\n",
        "    return"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgLLlCIyRS3J"
      },
      "source": [
        "\n",
        "def detect_emotions_webcam(model):\n",
        "    webcam = cv2.VideoCapture(0)\n",
        "\n",
        "    while True:\n",
        "        capture, frame = webcam.read()\n",
        "\n",
        "        if capture == False:\n",
        "            continue\n",
        "\n",
        "        # in video mode\n",
        "        display_expression(frame, model, mode=1)\n",
        "\n",
        "    return"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXMYLj8iRVLP",
        "outputId": "64294cb4-a963-4e98-f11b-1a1558382e6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "# Find facial expression in an individual img\n",
        "model = load_model(MODEL_PATH)\n",
        "path = \"./Test pics/got2.jpg\"\n",
        "full_img = cv2.imread(path)\n",
        "display_expression(full_img, model)\n",
        "\n",
        "# detect_emotions_webcam(model)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-308a363adf87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./Test pics/got2.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfull_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdisplay_expression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# detect_emotions_webcam(model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-fe9dddc74071>\u001b[0m in \u001b[0;36mdisplay_expression\u001b[0;34m(full_img, model, mode)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Use haarcascade to find face in img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mface_cascade\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"haarcascade_frontalface_default.xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_img_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Go over all faces found in image and detect emotion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/objdetect/src/cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'detectMultiScale'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2xvBoB9RXPX"
      },
      "source": [
        "# Test our models accuracy across our testing set\n",
        "test = False\n",
        "\n",
        "if test == True:\n",
        "    result=[]\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for emotion in os.listdir(TEST_DIR):\n",
        "\n",
        "        for filename in os.listdir(os.path.join(TEST_DIR, emotion)):\n",
        "            path = os.path.join(os.path.join(TEST_DIR,emotion), filename)\n",
        "            img = kimg.load_img(path, target_size=(48, 48), color_mode=\"grayscale\")\n",
        "\n",
        "            # models prediction\n",
        "            prediction = predict_emotion(img, model)\n",
        "            emotion_str = EMOTION_MAP[prediction[0]]\n",
        "\n",
        "            #if it matches true label\n",
        "            if emotion == emotion_str:\n",
        "                correct+=1\n",
        "\n",
        "            total += 1\n",
        "\n",
        "    print(\"test accuracy: {}\".format(correct/total))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}